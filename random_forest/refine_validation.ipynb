{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import dump\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix)\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from RefinedRandomForest import RefinedRandomForest"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "x_train = np.load('../feature_extraction/ResNext101/features_ResNext101_train.npy')\n",
    "y_train = np.load('../feature_extraction/ResNext101/labels_ResNext101_train.npy')\n",
    "x_valid = np.load('../feature_extraction/ResNext101/features_ResNext101_valid.npy')\n",
    "y_valid = np.load('../feature_extraction/ResNext101/labels_ResNext101_valid.npy')\n",
    "x_train = np.vstack((x_train, x_valid))\n",
    "y_train = np.vstack((y_train, y_valid))\n",
    "x_test = np.load('../feature_extraction/ResNext101/features_ResNext101_test.npy')\n",
    "y_test = np.load('../feature_extraction/ResNext101/labels_ResNext101_test.npy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "######### 29 random forest validation ###########\n",
    "rf_results = np.zeros_like(y_test)\n",
    "for i in range(y_test.shape[1]):\n",
    "    rfc = joblib.load(f'saved_model/rfc_{i}.joblib')\n",
    "    y_test_prob = rfc.predict_proba(x_test)\n",
    "    y_test_prob = np.array(y_test_prob)\n",
    "    pred = y_test_prob[:,1]\n",
    "    rf_results[:,i] = pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "auc1 = roc_auc_score(y_test[:,0], rf_results[:,0])\n",
    "print(f'AUC of Challenge 1: {auc1}')\n",
    "diseases_label = y_test[:,1:]\n",
    "diseases_pred = rf_results[:,1:]\n",
    "auc2 = roc_auc_score(diseases_label, diseases_pred)\n",
    "print(f'AUC of Challenge 2: {auc2}')\n",
    "mAP = average_precision_score(diseases_label, diseases_pred)\n",
    "print(f'mAP of Challenge 2: {mAP}')\n",
    "C1_Score = auc1\n",
    "C2_Score = mAP * 0.5 + auc2 * 0.5\n",
    "final_Score =  C2_Score * 0.5 + C1_Score * 0.5\n",
    "print(f'C1 Score: {C1_Score} C2 Score: {C2_Score} Final Score: {final_Score}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC of Challenge 1: 0.9736593711285471\n",
      "AUC of Challenge 2: 0.8654896513860499\n",
      "mAP of Challenge 2: 0.4997324385575243\n",
      "C1 Score: 0.9736593711285471 C2 Score: 0.6826110449717872 Final Score: 0.8281352080501672\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "######### 29 random forest validation ###########\n",
    "rrf_results = np.zeros_like(y_test)\n",
    "aucs = np.load('aucs.npy')\n",
    "optimal = aucs.argmax(axis=1)\n",
    "for i in range(y_test.shape[1]):\n",
    "    if optimal[i]==0:\n",
    "        rfc = joblib.load(f'saved_model/rfc_{i}.joblib')\n",
    "        y_test_prob = rfc.predict_proba(x_test)\n",
    "        y_test_prob = np.array(y_test_prob)\n",
    "        pred = y_test_prob[:,1]\n",
    "        rrf_results[:,i] = pred\n",
    "    else:\n",
    "        rrfc = joblib.load(f'saved_model/rrfc/rrfc_{i}.joblib')\n",
    "        pred = rrfc.predict_proba(x_test)[:,1]\n",
    "        rrf_results[:,i] = pred\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "auc1 = roc_auc_score(y_test[:,0], rrf_results[:,0])\n",
    "print(f'AUC of Challenge 1: {auc1}')\n",
    "diseases_label = y_test[:,1:]\n",
    "diseases_pred = rrf_results[:,1:]\n",
    "auc2 = roc_auc_score(diseases_label, diseases_pred)\n",
    "print(f'AUC of Challenge 2: {auc2}')\n",
    "mAP = average_precision_score(diseases_label, diseases_pred)\n",
    "print(f'mAP of Challenge 2: {mAP}')\n",
    "C1_Score = auc1\n",
    "C2_Score = mAP * 0.5 + auc2 * 0.5\n",
    "final_Score =  C2_Score * 0.5 + C1_Score * 0.5\n",
    "print(f'C1 Score: {C1_Score} C2 Score: {C2_Score} Final Score: {final_Score}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AUC of Challenge 1: 0.9736593711285471\n",
      "AUC of Challenge 2: 0.8996801448187943\n",
      "mAP of Challenge 2: 0.49453484839183764\n",
      "C1 Score: 0.9736593711285471 C2 Score: 0.697107496605316 Final Score: 0.8353834338669315\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "np.save('rf_results.npy', rf_results)\n",
    "np.save('rrf_result.npy', rrf_results)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "ec949caec1669b5a6f27961e507129f11fe2af4aae34c2ace3af081002d8b917"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}